{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6822cdb-53be-4130-99a6-36dd4774a6b4",
   "metadata": {},
   "source": [
    "# Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84fb3df2-7eb6-47f6-8b4e-17d534e4d307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelling\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn. preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e094b711-0f0d-4b68-86c1-61685d044c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data =pd.read_csv('preprocessed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae26e948-3591-474f-bedf-23c874e4b4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Evaluate Function to give all metrics after model Training\n",
    "def evaluate_model(true, predicted):\n",
    "    accuracy = accuracy_score(true, predicted)\n",
    "    precision = precision_score(true, predicted, average='weighted')  # or 'micro', 'macro', etc.\n",
    "    recall = recall_score(true, predicted, average='weighted')  # or 'micro', 'macro', etc.\n",
    "    f1 = f1_score(true, predicted, average='weighted')  # or 'micro', 'macro', etc.\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3edca22-e194-456c-aae8-b7161062a0b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "337115da-02c2-4eaa-a27d-aa5691271437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your classification models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, class_weight='balanced'),\n",
    "    \"Decision Tree Classifier\": DecisionTreeClassifier(),\n",
    "    \"Random Forest Classifier\": RandomForestClassifier(),\n",
    "    \"CatBoosting Classifier\": CatBoostClassifier(verbose=False),\n",
    "    \"AdaBoost Classifier\": AdaBoostClassifier(),\n",
    "    \"Support Vector Classifier\": SVC()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ffa77405-f0ca-4bdc-a8a1-a6f04d845fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define features and target\n",
    "x = data.drop(columns=['PerformanceRating'], axis=1)  # Replace with your actual target column\n",
    "y = data['PerformanceRating']\n",
    "\n",
    "# Now split the data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3993a5-e873-4f33-895c-07cf999a8e10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3ea7d67-f301-450b-8296-5589e94599a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Model performance for Training set\n",
      "- Accuracy: 0.7240\n",
      "- Precision: 0.8153\n",
      "- Recall: 0.7240\n",
      "- F1 Score: 0.7416\n",
      "----------------------------------\n",
      "Model performance for Test set\n",
      "- Accuracy: 0.7042\n",
      "- Precision: 0.8141\n",
      "- Recall: 0.7042\n",
      "- F1 Score: 0.7303\n",
      "===================================\n",
      "\n",
      "\n",
      "Decision Tree Classifier\n",
      "Model performance for Training set\n",
      "- Accuracy: 1.0000\n",
      "- Precision: 1.0000\n",
      "- Recall: 1.0000\n",
      "- F1 Score: 1.0000\n",
      "----------------------------------\n",
      "Model performance for Test set\n",
      "- Accuracy: 0.8958\n",
      "- Precision: 0.8996\n",
      "- Recall: 0.8958\n",
      "- F1 Score: 0.8969\n",
      "===================================\n",
      "\n",
      "\n",
      "Random Forest Classifier\n",
      "Model performance for Training set\n",
      "- Accuracy: 1.0000\n",
      "- Precision: 1.0000\n",
      "- Recall: 1.0000\n",
      "- F1 Score: 1.0000\n",
      "----------------------------------\n",
      "Model performance for Test set\n",
      "- Accuracy: 0.9375\n",
      "- Precision: 0.9402\n",
      "- Recall: 0.9375\n",
      "- F1 Score: 0.9367\n",
      "===================================\n",
      "\n",
      "\n",
      "CatBoosting Classifier\n",
      "Model performance for Training set\n",
      "- Accuracy: 1.0000\n",
      "- Precision: 1.0000\n",
      "- Recall: 1.0000\n",
      "- F1 Score: 1.0000\n",
      "----------------------------------\n",
      "Model performance for Test set\n",
      "- Accuracy: 0.9333\n",
      "- Precision: 0.9344\n",
      "- Recall: 0.9333\n",
      "- F1 Score: 0.9322\n",
      "===================================\n",
      "\n",
      "\n",
      "AdaBoost Classifier\n",
      "Model performance for Training set\n",
      "- Accuracy: 0.8708\n",
      "- Precision: 0.8671\n",
      "- Recall: 0.8708\n",
      "- F1 Score: 0.8669\n",
      "----------------------------------\n",
      "Model performance for Test set\n",
      "- Accuracy: 0.8750\n",
      "- Precision: 0.8702\n",
      "- Recall: 0.8750\n",
      "- F1 Score: 0.8694\n",
      "===================================\n",
      "\n",
      "\n",
      "Support Vector Classifier\n",
      "Model performance for Training set\n",
      "- Accuracy: 0.8969\n",
      "- Precision: 0.9091\n",
      "- Recall: 0.8969\n",
      "- F1 Score: 0.8885\n",
      "----------------------------------\n",
      "Model performance for Test set\n",
      "- Accuracy: 0.8292\n",
      "- Precision: 0.8252\n",
      "- Recall: 0.8292\n",
      "- F1 Score: 0.8031\n",
      "===================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "results = []\n",
    "\n",
    "# Iterate through the models and train/evaluate them\n",
    "for model_name, model in models.items():\n",
    "    # Use scaled data for models that require it\n",
    "    if name in [\"Logistic Regression\", \"Support Vector Classifier\"]:\n",
    "        model.fit(x_train_scaled, y_train)\n",
    "        y_train_pred = model.predict(x_train_scaled)\n",
    "        y_test_pred = model.predict(x_test_scaled)\n",
    "    else:\n",
    "        model.fit(x_train, y_train)\n",
    "        y_train_pred = model.predict(x_train)\n",
    "        y_test_pred = model.predict(x_test)\n",
    "\n",
    "    # Evaluate Train and Test dataset\n",
    "    accuracy_train, precision_train, recall_train, f1_train = evaluate_model(y_train, y_train_pred)\n",
    "    accuracy_test, precision_test, recall_test, f1_test = evaluate_model(y_test, y_test_pred)\n",
    "\n",
    "\n",
    "\n",
    "    print(model_name)\n",
    "    print('Model performance for Training set')\n",
    "    print(\"- Accuracy: {:.4f}\".format(accuracy_train))\n",
    "    print(\"- Precision: {:.4f}\".format(precision_train))\n",
    "    print(\"- Recall: {:.4f}\".format(recall_train))\n",
    "    print(\"- F1 Score: {:.4f}\".format(f1_train))\n",
    "    print('----------------------------------')\n",
    "    print('Model performance for Test set')\n",
    "    print(\"- Accuracy: {:.4f}\".format(accuracy_test))\n",
    "    print(\"- Precision: {:.4f}\".format(precision_test))\n",
    "    print(\"- Recall: {:.4f}\".format(recall_test))\n",
    "    print(\"- F1 Score: {:.4f}\".format(f1_test))\n",
    "    print('=' * 35)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43215c4c-95ad-42f3-a572-d96bdab1f17e",
   "metadata": {},
   "source": [
    "Based on the test set performance metrics, both the Random Forest Classifier and the CatBoosting Classifier seem to perform very well. They have high accuracy and F1 scores, indicating good balance between precision and recall. The Decision Tree Classifier also performs well but not as strongly as the other two."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fdb246-3162-4a91-958f-b8a7561a7ea5",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8357d815-21a8-4410-8703-d1f836a1d763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9333\n",
      "Precision: 0.9366\n",
      "Recall: 0.9333\n",
      "F1 Score: 0.9324\n"
     ]
    }
   ],
   "source": [
    "# Create a Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)  \n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_classifier.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = rf_classifier.predict(x_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Print the performance metrics\n",
    "print(\"Accuracy: {:.4f}\".format(accuracy))\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))\n",
    "print(\"F1 Score: {:.4f}\".format(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99872016-63c4-4b15-bca2-c0bb99372502",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
